{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "# courtesy of Guillaume Martin https://www.kaggle.com/gemartin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = import_data('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = import_data('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "corr = train_x.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "cmap = sns.diverging_palette(145, 280, s=85, l=15, n=100)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=.5,\n",
    "            cbar_kws={'shrink': .6})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.hist(train_x, figsize=(20, 20), bins=100)\n",
    "plt.show()plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=train_x['revives'], y=train_x['winPlacePerc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEngineer_train(dataset):\n",
    "    dataset.drop(['Id', 'groupId', 'matchId'], inplace=True, axis=1)  # ids\n",
    "    dataset.drop(['maxPlace'], inplace=True, axis=1)  # repeated variable from numGroups\n",
    "\n",
    "    dataset.drop('vehicleDestroys', axis=1, inplace=True)  # not relevant to skills\n",
    "\n",
    "    dataset['AllknockKillassist'] = dataset['DBNOs'] + dataset['assists'] + dataset['kills'] - dataset['roadKills']\n",
    "    dataset.drop(['DBNOs', 'assists', 'kills', 'killPlace', 'roadKills'], inplace=True, axis=1)  # essentially the same\n",
    "\n",
    "    dataset.drop('damageDealt', axis=1, inplace=True)  # over 0.8 correlation with kills/assist/DBNOs\n",
    "    \n",
    "    dataset['distanceTravelled'] = dataset['swimDistance'] + dataset['rideDistance'] + dataset['walkDistance']\n",
    "    dataset.drop(['swimDistance', 'rideDistance', 'walkDistance'], axis=1, inplace=True)  # combining distances travelled\n",
    "    \n",
    "    dataset['heal'] = dataset['heals'] + dataset['boosts']\n",
    "    dataset.drop(['heals', 'boosts'], axis=1, inplace=True)  # combining heals & boosts\n",
    "    \n",
    "    dataset.loc[dataset['revives'] > 10, 'revives'] = dataset.revives.median()  # anymore than that is just cheating/nonsense\n",
    "    \n",
    "    dataset.loc[dataset['teamKills'] > 0, 'teamKills'] = 1  # biased data, single matchmake default 0\n",
    "    dataset['teamKills'] = dataset['teamKills'].astype('category')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSelector(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return x[self.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleData = Pipeline([\n",
    "    ('selector', CustomSelector(['headshotKills', 'killPoints', 'killStreaks', 'longestKill', 'numGroups', 'revives', 'weaponsAcquired', 'winPoints', 'AllknockKillassist', 'distanceTravelled', 'heal'])),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y = train_x.pop('winPlacePerc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = featureEngineer_train(train_x)\n",
    "test_x = featureEngineer_train(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[['headshotKills', 'killPoints', 'killStreaks', 'longestKill', 'numGroups', 'revives', 'weaponsAcquired', 'winPoints', 'AllknockKillassist', 'distanceTravelled', 'heal']] = scaleData.fit_transform(train_x)\n",
    "test_x[['headshotKills', 'killPoints', 'killStreaks', 'longestKill', 'numGroups', 'revives', 'weaponsAcquired', 'winPoints', 'AllknockKillassist', 'distanceTravelled', 'heal']] = scaleData.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {'max_features': [8, 12]},\n",
    "    {'bootstrap': [False]}\n",
    "]  # running more params take a long time\n",
    "forest_reg = RandomForestRegressor(random_state=0, n_estimators=100)  # other hyperparameters will be default\n",
    "grid_search = GridSearchCV(forest_reg, params, cv=3, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_search.fit(train_x, train_y)\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in sorted(zip(cvres['mean_test_score'], cvres['params'])):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = grid_search.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['winPlacePerc'] = np.clip(prediction, 0, 1)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
